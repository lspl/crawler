## 起点小说信息

**目标**：爬取起点中文网“全部作品”的前100页信息（https://www.qidian.com/all），包括url, 小说名，作者，类型，完成情况，标签，摘要，字数

直接上版本1.0：xpath和xlwt的基本操作

```python
# -*- coding: utf-8 -*-
# 2018/4/6 21:37

import requests
import xlwt
import time
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3371.0 Safari/537.36'
}


def get_info(url0):
    res = requests.get(url0, headers=headers)
    selector = etree.HTML(res.text)
    infos = selector.xpath('//li[@data-rid]')
    info_list = []
    for info in infos:
        url1 = 'https:' + info.xpath('div[2]/h4/a/@href')[0]
        name = info.xpath('div[2]/h4/a/text()')[0]
        author = info.xpath('div[2]/p/a/text()')[0]
        type0 = info.xpath('div[2]/p/a[2]/text()')[0] + '.' + info.xpath('div[2]/p/a[3]/text()')[0]
        status = info.xpath('div[2]/p/span/text()')[0]
        tag = ''
        try:
            tag = info.xpath('div[2]/p/a[4]/text()')[0]
        except IndexError:
            pass
        words = info.xpath('div[2]/p[3]/span/span/text()')[0] + '万字'
        abstract = info.xpath('div[2]/p[2]/text()')[0].strip()
        info_list.append([url1, name, author, type0, status, tag, words, abstract])
    return info_list


if __name__ == '__main__':
    book = xlwt.Workbook(encoding='utf-8')
    sheet = book.add_sheet('Sheet1')
    title = ['url', 'name', 'author', 'type', 'status', 'tag', 'words', 'abstract']
    i = 0
    for h in range(len(title)):
        sheet.write(i, h, title[h])
    i = i + 1
    urls = ['https://www.qidian.com/all?orderId=&style=1&pageSize=20&siteid=1&pubflag=0&hiddenField=0&page={}'.format(
        str(i)) for i in range(1, 100)]
    for k in range(0, len(urls)):
        info0 = get_info(urls[k])
        for info in info0:
            for l in range(len(info)):
                sheet.write(i, l, info[l])
            i = i + 1
        time.sleep(1)
        print(k)
        k = k + 1
    book.save('temp.xls')
```

打开得到的temp.xls文件，发现如下情况：![字数显示为方框](https://raw.githubusercontent.com/lspl/crawler/master/6.%20%E8%B5%B7%E7%82%B9%E5%B0%8F%E8%AF%B4%E4%BF%A1%E6%81%AF/pic1.jpg)

什么情况，也没见过为方框的乱码啊。
仔细查看网页源代码，会发现一些有意思的东西：比如类似下面这种：“<span class="XspTnyat">&#100402;&#100397;&#100406;&#100400;&#100402;</span>万字</span>” 

没错，中间的“&#100402;&#100397;&#100406;&#100400;&#100402;”    表示的就是字数，只是[采用了一种映射的方法][1]。但并不是参考文献中的每天一变，而是大概1分钟内变一次。

好像可以用“全部作品“栏的数字进行映射得到真实的数字（动态，毕竟实际变化很快），看起来可以解决，实际并不可行，因为全部作品栏的作品正在更新中，它的字数会变。。

自然想到用已完成作品（https://www.qidian.com/finish）中的数字进行映射。前提是已完成作品中和全部作品中的数字映射方案相同。事实证明，确实一样。所以，得想办法凑齐’1‘，‘2’，‘3’，‘4’，‘5’，‘6’，‘7’，‘8’，‘9’，’0’，‘.’这11个字符，然后选中了这三个链接：'https://book.qidian.com/info/1006629321', 'https://book.qidian.com/info/1004992610',        'https://book.qidian.com/info/1003438608'，其字数分别为274.65,359.59,440.81，包含了上面的11个字符。只需要用爬虫分别找到这三个链接中对应数字的位置，然后进行数字的映射即可,也就是下面的mapping()函数。

于是就有了版本2.0：

```python
# -*- coding: utf-8 -*-

import requests
import xlwt
import time
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3371.0 Safari/537.36'
}


def bin_strs(num):
    s = num.encode('unicode_escape')
    s0 = str(s, encoding='utf-8')
    s1 = s0.split('\\U')
    s2 = s1[1:]
    return s2


def mapping():
    url0 = ['https://book.qidian.com/info/1006629321', 'https://book.qidian.com/info/1004992610',
            'https://book.qidian.com/info/1003438608']
    num0 = bin_strs(etree.HTML(requests.get(url0[0], headers=headers).text).xpath(
        '/html/body/div[2]/div[6]/div[1]/div[2]/p[3]/em[1]/span/text()')[0])
    num1 = bin_strs(etree.HTML(requests.get(url0[1], headers=headers).text).xpath(
        '/html/body/div[2]/div[6]/div[1]/div[2]/p[3]/em[1]/span/text()')[0])
    num2 = bin_strs(etree.HTML(requests.get(url0[2], headers=headers).text).xpath(
        '/html/body/div[2]/div[6]/div[1]/div[2]/p[3]/em[1]/span/text()')[0])
    num_mapping = {
        num2[2]: '0',
        num2[5]: '1',
        num0[0]: '2',
        num1[0]: '3',
        num0[2]: '4',
        num1[1]: '5',
        num0[4]: '6',
        num0[1]: '7',
        num2[4]: '8',
        num1[2]: '9',
        num0[3]: '.'
    }
    return num_mapping


def process(num, num_map):
    s = bin_strs(num)
    res = ''
    try:
        for j in range(0, len(s)):
            res = res + num_map[s[j]]
        return res
    except KeyError:
        raise KeyError("function process() key map error")


def get_info(url0):
    res = requests.get(url0, headers=headers)
    selector = etree.HTML(res.text)
    infos = selector.xpath('//li[@data-rid]')
    info_list = []
    num_map = mapping()
    for info in infos:
        url1 = 'https:' + info.xpath('div[2]/h4/a/@href')[0]
        name = info.xpath('div[2]/h4/a/text()')[0]
        author = info.xpath('div[2]/p/a/text()')[0]
        type0 = info.xpath('div[2]/p/a[2]/text()')[0] + '.' + info.xpath('div[2]/p/a[3]/text()')[0]
        status = info.xpath('div[2]/p/span/text()')[0]
        try:
            tag = info.xpath('div[2]/p/a[4]/text()')[0]
        except IndexError:
            tag = ''
        try:
            words = process(info.xpath('div[2]/p[3]/span/span/text()')[0], num_map) + '万字'
        except KeyError:
            words = ''
        abstract = info.xpath('div[2]/p[2]/text()')[0].strip()
        info_list.append([url1, name, author, type0, status, tag, words, abstract])
    return info_list


if __name__ == '__main__':
    book = xlwt.Workbook(encoding='utf-8')
    sheet = book.add_sheet('Sheet1')
    title = ['url', 'name', 'author', 'type', 'status', 'tag', 'words', 'abstract']
    i = 0
    for h in range(len(title)):
        sheet.write(i, h, title[h])
    i = i + 1
    urls = ['https://www.qidian.com/all?orderId=&style=1&pageSize=20&siteid=1&pubflag=0&hiddenField=0&page={}'.format(
        str(i)) for i in range(1, 100)]
    for k in range(0, len(urls)):
        info0 = get_info(urls[k])
        for info in info0:
            for l in range(len(info)):
                sheet.write(i, l, info[l])
            i = i + 1
        time.sleep(1)
        print(k)
        k = k + 1
    book.save('temp.xls')
```

解释一下：为了防止网页数字映射方案发生改变时，“全部作品”栏的方案和“完本”不同，选择抛出异常。当异常被接收到时，重新更改映射方案。

打开得到的temp.xls，发现大部分小说的字数都能取到，和网页的显示结果对比后也是正确的，至少说明方法还是可行的。但是，每次出现空白，就会有20个小说的字数为空。统计了下，发现字数为空的小说占比为10%（人品好的话可能低点）。个人猜测，主要原因是网速问题（4M。。）,导致网页爬取速度过慢编码对应不上，因为原始网页的数字编码在不停的变，如果不能在一次编码方案内将数字和解码对应上，自然就出错。

问题主要就是出在网页字数的获取上，因此可以考虑类似于数据库中事务的思想，将网页的爬取和字数的获取统一起来，一旦字数获取出现异常（得到的映射和实际的编码对应不上），就重新爬取当前网页，再次计算字数。直至OK。

因此有了版本3.0：

```python
# -*- coding: utf-8 -*-

import requests
import xlwt
import time
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3371.0 Safari/537.36'
}


def bin_strs(num):
    s = num.encode('unicode_escape')
    s0 = str(s, encoding='utf-8')
    s1 = s0.split('\\U')
    s2 = s1[1:]
    return s2


def mapping():
    url0 = ['https://book.qidian.com/info/1006629321', 'https://book.qidian.com/info/1004992610',
            'https://book.qidian.com/info/1003438608']
    num0 = bin_strs(etree.HTML(requests.get(url0[0], headers=headers).text).xpath(
        '/html/body/div[2]/div[6]/div[1]/div[2]/p[3]/em[1]/span/text()')[0])
    num1 = bin_strs(etree.HTML(requests.get(url0[1], headers=headers).text).xpath(
        '/html/body/div[2]/div[6]/div[1]/div[2]/p[3]/em[1]/span/text()')[0])
    num2 = bin_strs(etree.HTML(requests.get(url0[2], headers=headers).text).xpath(
        '/html/body/div[2]/div[6]/div[1]/div[2]/p[3]/em[1]/span/text()')[0])
    num_mapping = {
        num2[2]: '0',
        num2[5]: '1',
        num0[0]: '2',
        num1[0]: '3',
        num0[2]: '4',
        num1[1]: '5',
        num0[4]: '6',
        num0[1]: '7',
        num2[4]: '8',
        num1[2]: '9',
        num0[3]: '.'
    }
    return num_mapping


def process(num, num_map):
    s = bin_strs(num)
    res = ''
    try:
        for j in range(0, len(s)):
            res = res + num_map[s[j]]
        return res
    except KeyError:
        raise KeyError("function process() key map error")


def get_info(url0):
    while True:
        try:
            res = requests.get(url0, headers=headers)
            selector = etree.HTML(res.text)
            infos = selector.xpath('//li[@data-rid]')
            info_list = []
            num_map = mapping()
            words = []
            for index in range(0, len(infos)):
                words.append(process(infos[index].xpath('div[2]/p[3]/span/span/text()')[0], num_map) + '万字')
            break
        except KeyError:
            time.sleep(5)
            pass

    for l in range(0, len(infos)):
        url1 = 'https:' + infos[l].xpath('div[2]/h4/a/@href')[0]
        name = infos[l].xpath('div[2]/h4/a/text()')[0]
        author = infos[l].xpath('div[2]/p/a/text()')[0]
        type0 = infos[l].xpath('div[2]/p/a[2]/text()')[0] + '.' + infos[l].xpath('div[2]/p/a[3]/text()')[0]
        status = infos[l].xpath('div[2]/p/span/text()')[0]
        try:
            tag = infos[l].xpath('div[2]/p/a[4]/text()')[0]
        except IndexError:
            tag = ''
        abstract = infos[l].xpath('div[2]/p[2]/text()')[0].strip()
        info_list.append([url1, name, author, type0, status, tag, words[l], abstract])
    return info_list


if __name__ == '__main__':
    book = xlwt.Workbook(encoding='utf-8')
    sheet = book.add_sheet('Sheet1')
    title = ['url', 'name', 'author', 'type', 'status', 'tag', 'words', 'abstract']
    i = 0
    for h in range(len(title)):
        sheet.write(i, h, title[h])
    i = i + 1
    urls = ['https://www.qidian.com/all?orderId=&style=1&pageSize=20&siteid=1&pubflag=0&hiddenField=0&page={}'.format(
        str(i)) for i in range(1, 101)]
    for k in range(0, len(urls)):
        info0 = get_info(urls[k])
        for info in info0:
            for l in range(len(info)):
                sheet.write(i, l, info[l])
            i = i + 1
        time.sleep(1)
        print(k)
        k = k + 1
    book.save('temp.xls')
```

愉快的解决问题(\*^▽^\*)

[1]: https://zhuanlan.zhihu.com/p/35272756